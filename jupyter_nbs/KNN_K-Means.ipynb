{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Gtk3 backend requires pygobject to be installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7b40dc38a39d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Python packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab_setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0m_backend_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_figure_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_if_interactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpylab_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.pyc\u001b[0m in \u001b[0;36mpylab_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# imports. 0 means only perform absolute imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     backend_mod = __import__(backend_name,\n\u001b[0;32m---> 32\u001b[0;31m                              globals(),locals(),[backend_name],0)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Things we pull in from all backends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/backends/backend_gtk3agg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_agg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_gtk3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend_cairo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcairo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHAS_CAIRO_CFFI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/backends/backend_gtk3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mgi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gtk3 backend requires pygobject to be installed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Gtk3 backend requires pygobject to be installed."
     ]
    }
   ],
   "source": [
    "# Python packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import signal\n",
    "import argparse\n",
    "import traceback\n",
    "import json\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bcf8ec7eb652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Project libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../src/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mebd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "# Project libraries\n",
    "sys.path.insert(1, '../src/')\n",
    "\n",
    "import embedding.factory as ebd\n",
    "import dataset.loader as loader\n",
    "import train.factory as train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': '../data/20news.json', 'dataset': '20newsgroup', 'n_train_class': 8, 'n_val_class': 5, 'n_test_class': 7, 'mode': 'test', 'wv_path': '../', 'word_vector': 'wiki.en.vec', 'finetune_ebd': True, 'bert': False, 'auxiliary': [], 'embedding': 'avg', 'meta_w_target': False, 'cuda': 0, 'snapshot': ''}\n"
     ]
    }
   ],
   "source": [
    "# Build args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_path\", type=str,\n",
    "                        default=\"data/20news.json\", # og: reuters\n",
    "                        help=\"path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"20newsgroup\", # og: reuters\n",
    "                    help=\"name of the dataset. \"\n",
    "                    \"Options: [20newsgroup, amazon, huffpost, \"\n",
    "                    \"reuters, rcv1, fewrel]\")\n",
    "parser.add_argument(\"--n_train_class\", type=int, default=15,\n",
    "                    help=\"number of meta-train classes\")\n",
    "parser.add_argument(\"--n_val_class\", type=int, default=5,\n",
    "                    help=\"number of meta-val classes\")\n",
    "parser.add_argument(\"--n_test_class\", type=int, default=11,\n",
    "                    help=\"number of meta-test classes\")\n",
    "parser.add_argument(\"--mode\", type=str, default=\"test\",\n",
    "                    help=(\"Running mode.\"\n",
    "                          \"Options: [train, test, finetune]\"\n",
    "                          \"[Default: test]\"))\n",
    "parser.add_argument(\"--wv_path\", type=str,\n",
    "                    default=\"./\",\n",
    "                    help=\"path to word vector cache\")\n",
    "parser.add_argument(\"--word_vector\", type=str, default=\"wiki.en.vec\",\n",
    "                    help=(\"Name of pretrained word embeddings.\"))\n",
    "parser.add_argument(\"--finetune_ebd\", action=\"store_true\", default=False,\n",
    "                    help=(\"Finetune embedding during meta-training\"))\n",
    "parser.add_argument(\"--bert\", default=False, action=\"store_true\",\n",
    "                    help=(\"set true if use bert embeddings \"\n",
    "                          \"(only available for sent-level datasets: \"\n",
    "                          \"huffpost, fewrel\"))\n",
    "parser.add_argument(\"--auxiliary\", type=str, nargs=\"*\", default=[],\n",
    "                    help=(\"auxiliary embeddings (used for fewrel). \"\n",
    "                          \"Options: [pos, ent]\"))\n",
    "parser.add_argument(\"--embedding\", type=str, default=\"avg\",\n",
    "                    help=(\"document embedding method. Options: \"\n",
    "                          \"[avg, tfidf, meta, oracle, cnn]\"))\n",
    "parser.add_argument(\"--meta_w_target\", action=\"store_true\", default=False,\n",
    "                    help=\"use target importance score\")\n",
    "parser.add_argument(\"--cuda\", type=int, default=-1,\n",
    "                    help=\"cuda device, -1 for cpu\")\n",
    "parser.add_argument(\"--snapshot\", type=str, default=\"\",\n",
    "                    help=\"path to the pretraiend weights\")\n",
    "\n",
    "\n",
    "# Populate parameters\n",
    "args = parser.parse_args([\"--data_path\", \"../data/20news.json\",\n",
    "                            \"--dataset\", \"20newsgroup\",\n",
    "                            \"--n_train_class\", \"8\",\n",
    "                            \"--n_val_class\", \"5\",\n",
    "                            \"--n_test_class\", \"7\",\n",
    "                            \"--wv_path\", \"../\",\n",
    "                            \"--cuda\", \"0\",\n",
    "                            \"--finetune\"\n",
    "                            ])\n",
    "\n",
    "print(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/30 13:50:24: Loading data from ../data/20news.json\n",
      "20/03/30 13:50:25: Class balance:\n",
      "{0: 940, 1: 987, 2: 972, 3: 775, 4: 973, 5: 991, 6: 980, 7: 985, 8: 910, 9: 628, 10: 990, 11: 990, 12: 961, 13: 981, 14: 999, 15: 799, 16: 994, 17: 982, 18: 994, 19: 997}\n",
      "20/03/30 13:50:25: Avg len: 340.86015508816655\n",
      "20/03/30 13:50:25: Loading word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 13:50:25.084720 140443841652544 vocab.py:431] Loading vectors from ../wiki.en.vec.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/30 13:50:29: Total num. of words: 32137, word vector dimension: 300\n",
      "20/03/30 13:50:29: Num. of out-of-vocabulary words(they are initialized to zeros): 9095\n",
      "20/03/30 13:50:29: #train 7926, #val 4881, #test 6021\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, vocab = loader.load_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 5, 10, 11, 13, 14, 16, 18}\n"
     ]
    }
   ],
   "source": [
    "print(set(train_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 2, 3, 8, 9, 15, 19}\n"
     ]
    }
   ],
   "source": [
    "print(set(test_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 18828 documents, 20 different labels and 32137 size vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Aggregate all training, val, test data\n",
    "data = {}\n",
    "\n",
    "for key in test_data:\n",
    "    if key == 'vocab_size':\n",
    "        data['vocab_size'] = test_data[key]\n",
    "        continue\n",
    "        \n",
    "    ax = 0 if key == 'text' else None\n",
    "    concat = np.concatenate(\n",
    "            (test_data[key], val_data[key], train_data[key]), axis=ax\n",
    "        )\n",
    "    \n",
    "    if key == 'text':\n",
    "        data[key] = torch.tensor(concat, device=cuda0)\n",
    "    else:\n",
    "        data[key] = concat\n",
    "\n",
    "assert len(data['text']) == len(test_data['text']) + len(train_data['text']) + len(val_data['text'])\n",
    "\n",
    "print('Dataset has {} documents, {} different labels and {} size vocabulary.'.format(\n",
    "    len(data['text']), max(data['label']+1), data['vocab_size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'text_len', 'label', 'raw', 'vocab_size'])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n"
     ]
    }
   ],
   "source": [
    "print(set(data['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to memory constraints on the GPU, create a data \"queue\" of chunks of user-specified size, and embed the data one chunk (several documents) at a time. Each row in each resulting tensor corresponds to the document embedding (AVG, WORDEBD) of a document. We can access the original labels in __data__ and the tensors in __embed_queue__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(chunk_size, data):\n",
    "    \"\"\"\n",
    "    Chunks data into manageable sizes for processing\n",
    "    \n",
    "    Args:\n",
    "        chunk_size (int): number of words for each chunk\n",
    "        data (dict): data dictionary w keys 'text', 'vocab_size', etc\n",
    "    \n",
    "    Returns:\n",
    "        list of dictionaries in order\n",
    "    \"\"\"\n",
    "    chunk_list = []\n",
    "    total_size = len(data['text'])\n",
    "    cumu_size = 0\n",
    "    \n",
    "    while cumu_size < total_size:\n",
    "        next_size = min(cumu_size + chunk_size, total_size)\n",
    "        \n",
    "        # Create chunk\n",
    "        chunk = {\n",
    "            'text' : torch.tensor(data['text'][cumu_size:next_size], device=cuda0),\n",
    "            'text_len' : torch.tensor(data['text_len'][cumu_size:next_size], device=cuda0),\n",
    "            'label' : torch.tensor(data['label'][cumu_size:next_size], device=cuda0),\n",
    "            'raw' : data['raw'][cumu_size:next_size],\n",
    "            'vocab_size' : data['vocab_size']\n",
    "        }\n",
    "\n",
    "        chunk_list.append(chunk)\n",
    "        \n",
    "        # Update cumulative size\n",
    "        cumu_size = next_size\n",
    "        \n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/30 13:50:48, Building embedding\n",
      "Embedding type: WORDEBD\n",
      "Using:  avg\n",
      "20/03/30 13:50:48, Building embedding\n"
     ]
    }
   ],
   "source": [
    "data_queue = chunk_data(50, data)\n",
    "tr_data_queue = chunk_data(50, train_data)\n",
    "te_data_queue = chunk_data(50, test_data)\n",
    "\n",
    "model = {}\n",
    "model[\"ebd\"] = ebd.get_embedding(vocab, args)\n",
    "embed = model['ebd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded documents queue\n",
    "tr_embed_queue = [embed(chunk) for chunk in tr_data_queue]\n",
    "te_embed_queue = [embed(chunk) for chunk in te_data_queue]\n",
    "embed_queue = [embed(chunk) for chunk in data_queue]\n",
    "\n",
    "\n",
    "# Combine the embeddings together and create field in data dict\n",
    "train_data['ebd'] = torch.cat(tr_embed_queue).cpu().detach().numpy()\n",
    "test_data['ebd'] = torch.cat(te_embed_queue).cpu().detach().numpy()\n",
    "data['ebd'] = torch.cat(embed_queue).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.124415</td>\n",
       "      <td>-0.020417</td>\n",
       "      <td>-0.069346</td>\n",
       "      <td>0.091377</td>\n",
       "      <td>-0.079796</td>\n",
       "      <td>0.070282</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>-0.216919</td>\n",
       "      <td>-0.046902</td>\n",
       "      <td>0.129846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>-0.090170</td>\n",
       "      <td>0.152281</td>\n",
       "      <td>-0.074729</td>\n",
       "      <td>-0.144545</td>\n",
       "      <td>0.105670</td>\n",
       "      <td>0.115511</td>\n",
       "      <td>0.104688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.075159</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>-0.099258</td>\n",
       "      <td>0.112942</td>\n",
       "      <td>-0.101508</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>0.022061</td>\n",
       "      <td>-0.135841</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.187534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043346</td>\n",
       "      <td>-0.013045</td>\n",
       "      <td>0.003885</td>\n",
       "      <td>-0.000979</td>\n",
       "      <td>0.027354</td>\n",
       "      <td>-0.040816</td>\n",
       "      <td>-0.111924</td>\n",
       "      <td>0.128747</td>\n",
       "      <td>0.056314</td>\n",
       "      <td>0.046071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.080389</td>\n",
       "      <td>-0.041529</td>\n",
       "      <td>-0.061312</td>\n",
       "      <td>0.103755</td>\n",
       "      <td>-0.086603</td>\n",
       "      <td>0.010311</td>\n",
       "      <td>0.038585</td>\n",
       "      <td>-0.187294</td>\n",
       "      <td>-0.017839</td>\n",
       "      <td>0.148724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020543</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>0.007602</td>\n",
       "      <td>-0.025330</td>\n",
       "      <td>0.072066</td>\n",
       "      <td>-0.079325</td>\n",
       "      <td>-0.086958</td>\n",
       "      <td>0.107416</td>\n",
       "      <td>0.077540</td>\n",
       "      <td>0.039650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.022361</td>\n",
       "      <td>-0.077752</td>\n",
       "      <td>-0.039378</td>\n",
       "      <td>0.090882</td>\n",
       "      <td>-0.096249</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.028690</td>\n",
       "      <td>-0.152463</td>\n",
       "      <td>0.054856</td>\n",
       "      <td>0.201151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024943</td>\n",
       "      <td>-0.064121</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.011391</td>\n",
       "      <td>0.035032</td>\n",
       "      <td>-0.111200</td>\n",
       "      <td>-0.067736</td>\n",
       "      <td>0.138786</td>\n",
       "      <td>0.059619</td>\n",
       "      <td>0.019695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.081447</td>\n",
       "      <td>-0.007211</td>\n",
       "      <td>-0.073106</td>\n",
       "      <td>0.097729</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>0.016781</td>\n",
       "      <td>-0.019609</td>\n",
       "      <td>-0.190727</td>\n",
       "      <td>0.014717</td>\n",
       "      <td>0.153712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015522</td>\n",
       "      <td>0.041751</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>-0.079508</td>\n",
       "      <td>0.083521</td>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.114408</td>\n",
       "      <td>0.143006</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>-0.018870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.095174</td>\n",
       "      <td>-0.013247</td>\n",
       "      <td>-0.077830</td>\n",
       "      <td>0.119162</td>\n",
       "      <td>-0.073894</td>\n",
       "      <td>0.010618</td>\n",
       "      <td>0.018167</td>\n",
       "      <td>-0.195425</td>\n",
       "      <td>-0.012059</td>\n",
       "      <td>0.182049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024403</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>-0.010742</td>\n",
       "      <td>-0.043991</td>\n",
       "      <td>0.058883</td>\n",
       "      <td>-0.080037</td>\n",
       "      <td>-0.106839</td>\n",
       "      <td>0.123544</td>\n",
       "      <td>0.080793</td>\n",
       "      <td>0.036533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.069868</td>\n",
       "      <td>-0.047623</td>\n",
       "      <td>-0.095615</td>\n",
       "      <td>0.153674</td>\n",
       "      <td>-0.068575</td>\n",
       "      <td>0.034050</td>\n",
       "      <td>0.026899</td>\n",
       "      <td>-0.179638</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.188709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>-0.013060</td>\n",
       "      <td>0.021143</td>\n",
       "      <td>0.010844</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>-0.066462</td>\n",
       "      <td>-0.110029</td>\n",
       "      <td>0.121081</td>\n",
       "      <td>0.030960</td>\n",
       "      <td>0.036877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.077197</td>\n",
       "      <td>-0.099006</td>\n",
       "      <td>-0.067128</td>\n",
       "      <td>0.148165</td>\n",
       "      <td>-0.098624</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>0.008062</td>\n",
       "      <td>-0.162496</td>\n",
       "      <td>-0.013794</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031426</td>\n",
       "      <td>-0.050556</td>\n",
       "      <td>0.042869</td>\n",
       "      <td>0.029839</td>\n",
       "      <td>0.036257</td>\n",
       "      <td>-0.068043</td>\n",
       "      <td>-0.103222</td>\n",
       "      <td>0.081063</td>\n",
       "      <td>0.069935</td>\n",
       "      <td>0.033167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.082979</td>\n",
       "      <td>-0.050510</td>\n",
       "      <td>-0.045994</td>\n",
       "      <td>0.121490</td>\n",
       "      <td>-0.070642</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.008936</td>\n",
       "      <td>-0.173120</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>0.140621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067901</td>\n",
       "      <td>-0.045484</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.032102</td>\n",
       "      <td>0.044224</td>\n",
       "      <td>-0.060718</td>\n",
       "      <td>-0.081409</td>\n",
       "      <td>0.107732</td>\n",
       "      <td>0.050481</td>\n",
       "      <td>0.065085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.051926</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.080794</td>\n",
       "      <td>0.126547</td>\n",
       "      <td>-0.104622</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.016536</td>\n",
       "      <td>-0.123840</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.174649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046032</td>\n",
       "      <td>-0.029945</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>-0.007207</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>-0.033420</td>\n",
       "      <td>-0.100970</td>\n",
       "      <td>0.114844</td>\n",
       "      <td>0.060479</td>\n",
       "      <td>0.047115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.124415 -0.020417 -0.069346  0.091377 -0.079796  0.070282  0.075804   \n",
       "1  -0.075159 -0.003043 -0.099258  0.112942 -0.101508  0.008385  0.022061   \n",
       "2  -0.080389 -0.041529 -0.061312  0.103755 -0.086603  0.010311  0.038585   \n",
       "3  -0.022361 -0.077752 -0.039378  0.090882 -0.096249  0.005133  0.028690   \n",
       "4  -0.081447 -0.007211 -0.073106  0.097729 -0.103240  0.016781 -0.019609   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95 -0.095174 -0.013247 -0.077830  0.119162 -0.073894  0.010618  0.018167   \n",
       "96 -0.069868 -0.047623 -0.095615  0.153674 -0.068575  0.034050  0.026899   \n",
       "97 -0.077197 -0.099006 -0.067128  0.148165 -0.098624  0.018120  0.008062   \n",
       "98 -0.082979 -0.050510 -0.045994  0.121490 -0.070642  0.000380  0.008936   \n",
       "99 -0.051926 -0.026389 -0.080794  0.126547 -0.104622  0.000592  0.016536   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "0  -0.216919 -0.046902  0.129846  ...  0.010435  0.052237  0.007903 -0.090170   \n",
       "1  -0.135841  0.004797  0.187534  ...  0.043346 -0.013045  0.003885 -0.000979   \n",
       "2  -0.187294 -0.017839  0.148724  ...  0.020543 -0.000655  0.007602 -0.025330   \n",
       "3  -0.152463  0.054856  0.201151  ...  0.024943 -0.064121  0.042752  0.011391   \n",
       "4  -0.190727  0.014717  0.153712  ...  0.015522  0.041751  0.010705 -0.079508   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95 -0.195425 -0.012059  0.182049  ...  0.024403  0.023333 -0.010742 -0.043991   \n",
       "96 -0.179638  0.004796  0.188709  ...  0.041883 -0.013060  0.021143  0.010844   \n",
       "97 -0.162496 -0.013794  0.157968  ...  0.031426 -0.050556  0.042869  0.029839   \n",
       "98 -0.173120  0.015073  0.140621  ...  0.067901 -0.045484  0.006674  0.032102   \n",
       "99 -0.123840  0.001218  0.174649  ...  0.046032 -0.029945  0.008532 -0.007207   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "0   0.152281 -0.074729 -0.144545  0.105670  0.115511  0.104688  \n",
       "1   0.027354 -0.040816 -0.111924  0.128747  0.056314  0.046071  \n",
       "2   0.072066 -0.079325 -0.086958  0.107416  0.077540  0.039650  \n",
       "3   0.035032 -0.111200 -0.067736  0.138786  0.059619  0.019695  \n",
       "4   0.083521 -0.123763 -0.114408  0.143006  0.100094 -0.018870  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.058883 -0.080037 -0.106839  0.123544  0.080793  0.036533  \n",
       "96  0.025940 -0.066462 -0.110029  0.121081  0.030960  0.036877  \n",
       "97  0.036257 -0.068043 -0.103222  0.081063  0.069935  0.033167  \n",
       "98  0.044224 -0.060718 -0.081409  0.107732  0.050481  0.065085  \n",
       "99  0.015924 -0.033420 -0.100970  0.114844  0.060479  0.047115  \n",
       "\n",
       "[100 rows x 300 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.DataFrame(data['ebd'])\n",
    "all_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.071192</td>\n",
       "      <td>-0.012361</td>\n",
       "      <td>-0.085839</td>\n",
       "      <td>0.136673</td>\n",
       "      <td>-0.088308</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>-0.143944</td>\n",
       "      <td>-0.026524</td>\n",
       "      <td>0.135350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062775</td>\n",
       "      <td>-0.018588</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>-0.019892</td>\n",
       "      <td>0.041033</td>\n",
       "      <td>-0.107942</td>\n",
       "      <td>-0.057266</td>\n",
       "      <td>0.129144</td>\n",
       "      <td>0.099781</td>\n",
       "      <td>0.061606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041846</td>\n",
       "      <td>-0.024256</td>\n",
       "      <td>-0.079570</td>\n",
       "      <td>0.159294</td>\n",
       "      <td>-0.090971</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>-0.033902</td>\n",
       "      <td>-0.139918</td>\n",
       "      <td>0.029066</td>\n",
       "      <td>0.116877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>-0.111922</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.036722</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>-0.125525</td>\n",
       "      <td>-0.087844</td>\n",
       "      <td>0.148737</td>\n",
       "      <td>0.047283</td>\n",
       "      <td>0.021140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.070470</td>\n",
       "      <td>0.009819</td>\n",
       "      <td>-0.071082</td>\n",
       "      <td>0.150578</td>\n",
       "      <td>-0.086470</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>-0.165744</td>\n",
       "      <td>-0.010373</td>\n",
       "      <td>0.129809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057856</td>\n",
       "      <td>-0.029996</td>\n",
       "      <td>0.034615</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.032689</td>\n",
       "      <td>-0.107052</td>\n",
       "      <td>-0.077811</td>\n",
       "      <td>0.157702</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>0.052546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.071521</td>\n",
       "      <td>-0.008882</td>\n",
       "      <td>-0.042624</td>\n",
       "      <td>0.132055</td>\n",
       "      <td>-0.091530</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.027034</td>\n",
       "      <td>-0.151465</td>\n",
       "      <td>-0.012600</td>\n",
       "      <td>0.132218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058395</td>\n",
       "      <td>-0.001610</td>\n",
       "      <td>0.017042</td>\n",
       "      <td>0.058495</td>\n",
       "      <td>0.050868</td>\n",
       "      <td>-0.132523</td>\n",
       "      <td>-0.080150</td>\n",
       "      <td>0.126434</td>\n",
       "      <td>0.072268</td>\n",
       "      <td>0.015644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.085329</td>\n",
       "      <td>-0.015131</td>\n",
       "      <td>-0.050789</td>\n",
       "      <td>0.117801</td>\n",
       "      <td>-0.070769</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.022904</td>\n",
       "      <td>-0.120538</td>\n",
       "      <td>0.052579</td>\n",
       "      <td>0.066595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>-0.101711</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>0.039110</td>\n",
       "      <td>-0.077845</td>\n",
       "      <td>-0.125016</td>\n",
       "      <td>0.133442</td>\n",
       "      <td>0.115185</td>\n",
       "      <td>0.027180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.083635</td>\n",
       "      <td>-0.028580</td>\n",
       "      <td>-0.080969</td>\n",
       "      <td>0.130626</td>\n",
       "      <td>-0.105474</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>-0.004547</td>\n",
       "      <td>-0.154977</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>0.141302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044397</td>\n",
       "      <td>-0.013539</td>\n",
       "      <td>0.010918</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>0.057695</td>\n",
       "      <td>-0.074778</td>\n",
       "      <td>-0.058086</td>\n",
       "      <td>0.143707</td>\n",
       "      <td>0.097171</td>\n",
       "      <td>0.047469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.078949</td>\n",
       "      <td>-0.003455</td>\n",
       "      <td>-0.064119</td>\n",
       "      <td>0.105644</td>\n",
       "      <td>-0.087107</td>\n",
       "      <td>0.010494</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>-0.153609</td>\n",
       "      <td>-0.013248</td>\n",
       "      <td>0.165260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059539</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>0.016921</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.045486</td>\n",
       "      <td>-0.096417</td>\n",
       "      <td>-0.088853</td>\n",
       "      <td>0.137824</td>\n",
       "      <td>0.068355</td>\n",
       "      <td>0.061161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.066152</td>\n",
       "      <td>-0.014906</td>\n",
       "      <td>-0.034030</td>\n",
       "      <td>0.082242</td>\n",
       "      <td>-0.089661</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.031910</td>\n",
       "      <td>-0.156900</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.107461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066905</td>\n",
       "      <td>-0.046841</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>0.063436</td>\n",
       "      <td>-0.086365</td>\n",
       "      <td>-0.104695</td>\n",
       "      <td>0.121863</td>\n",
       "      <td>0.113497</td>\n",
       "      <td>0.045664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.130918</td>\n",
       "      <td>-0.148796</td>\n",
       "      <td>0.120990</td>\n",
       "      <td>0.047321</td>\n",
       "      <td>-0.138596</td>\n",
       "      <td>-0.084153</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>-0.043948</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.087553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090146</td>\n",
       "      <td>-0.083179</td>\n",
       "      <td>0.058428</td>\n",
       "      <td>0.044482</td>\n",
       "      <td>0.086092</td>\n",
       "      <td>-0.151272</td>\n",
       "      <td>0.025468</td>\n",
       "      <td>0.029642</td>\n",
       "      <td>0.067901</td>\n",
       "      <td>0.099497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.086677</td>\n",
       "      <td>-0.057102</td>\n",
       "      <td>-0.024619</td>\n",
       "      <td>0.090777</td>\n",
       "      <td>-0.086502</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.014265</td>\n",
       "      <td>-0.133520</td>\n",
       "      <td>0.040727</td>\n",
       "      <td>0.111349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052334</td>\n",
       "      <td>-0.087713</td>\n",
       "      <td>0.019624</td>\n",
       "      <td>0.039313</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>-0.077985</td>\n",
       "      <td>-0.054337</td>\n",
       "      <td>0.146017</td>\n",
       "      <td>0.078478</td>\n",
       "      <td>0.053597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.071192 -0.012361 -0.085839  0.136673 -0.088308  0.000446  0.011034   \n",
       "1  -0.041846 -0.024256 -0.079570  0.159294 -0.090971  0.018244 -0.033902   \n",
       "2  -0.070470  0.009819 -0.071082  0.150578 -0.086470  0.033564  0.001022   \n",
       "3  -0.071521 -0.008882 -0.042624  0.132055 -0.091530  0.002158  0.027034   \n",
       "4  -0.085329 -0.015131 -0.050789  0.117801 -0.070769  0.011131  0.022904   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95 -0.083635 -0.028580 -0.080969  0.130626 -0.105474  0.020523 -0.004547   \n",
       "96 -0.078949 -0.003455 -0.064119  0.105644 -0.087107  0.010494  0.000608   \n",
       "97 -0.066152 -0.014906 -0.034030  0.082242 -0.089661  0.009507  0.031910   \n",
       "98 -0.130918 -0.148796  0.120990  0.047321 -0.138596 -0.084153  0.004985   \n",
       "99 -0.086677 -0.057102 -0.024619  0.090777 -0.086502  0.003947  0.014265   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "0  -0.143944 -0.026524  0.135350  ...  0.062775 -0.018588  0.000405 -0.019892   \n",
       "1  -0.139918  0.029066  0.116877  ...  0.079000 -0.111922  0.022248  0.036722   \n",
       "2  -0.165744 -0.010373  0.129809  ...  0.057856 -0.029996  0.034615  0.004137   \n",
       "3  -0.151465 -0.012600  0.132218  ...  0.058395 -0.001610  0.017042  0.058495   \n",
       "4  -0.120538  0.052579  0.066595  ...  0.055565 -0.101711  0.013240  0.004888   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95 -0.154977 -0.008059  0.141302  ...  0.044397 -0.013539  0.010918 -0.005697   \n",
       "96 -0.153609 -0.013248  0.165260  ...  0.059539 -0.012643  0.016921  0.007850   \n",
       "97 -0.156900  0.010539  0.107461  ...  0.066905 -0.046841  0.022918  0.019186   \n",
       "98 -0.043948 -0.000484  0.087553  ...  0.090146 -0.083179  0.058428  0.044482   \n",
       "99 -0.133520  0.040727  0.111349  ...  0.052334 -0.087713  0.019624  0.039313   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "0   0.041033 -0.107942 -0.057266  0.129144  0.099781  0.061606  \n",
       "1   0.001079 -0.125525 -0.087844  0.148737  0.047283  0.021140  \n",
       "2   0.032689 -0.107052 -0.077811  0.157702  0.092626  0.052546  \n",
       "3   0.050868 -0.132523 -0.080150  0.126434  0.072268  0.015644  \n",
       "4   0.039110 -0.077845 -0.125016  0.133442  0.115185  0.027180  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.057695 -0.074778 -0.058086  0.143707  0.097171  0.047469  \n",
       "96  0.045486 -0.096417 -0.088853  0.137824  0.068355  0.061161  \n",
       "97  0.063436 -0.086365 -0.104695  0.121863  0.113497  0.045664  \n",
       "98  0.086092 -0.151272  0.025468  0.029642  0.067901  0.099497  \n",
       "99  0.007817 -0.077985 -0.054337  0.146017  0.078478  0.053597  \n",
       "\n",
       "[100 rows x 300 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show embedding df\n",
    "tr_df = pd.DataFrame(train_data['ebd'])\n",
    "\n",
    "tr_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_df = pd.DataFrame(test_data['ebd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Topic Embed Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../20news_reps_cache_.json') as json_file:\n",
    "                topics = json.load(json_file) \n",
    "topics = pd.DataFrame.from_dict(topics, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.152460</td>\n",
       "      <td>-0.590110</td>\n",
       "      <td>0.079915</td>\n",
       "      <td>0.333420</td>\n",
       "      <td>-0.088485</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>-0.114920</td>\n",
       "      <td>-0.579780</td>\n",
       "      <td>-0.036263</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013480</td>\n",
       "      <td>-0.017638</td>\n",
       "      <td>0.620890</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>0.052670</td>\n",
       "      <td>0.565990</td>\n",
       "      <td>-0.065357</td>\n",
       "      <td>-0.507710</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.155420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.152330</td>\n",
       "      <td>0.018883</td>\n",
       "      <td>0.248440</td>\n",
       "      <td>0.308760</td>\n",
       "      <td>0.321930</td>\n",
       "      <td>0.103750</td>\n",
       "      <td>-0.057328</td>\n",
       "      <td>-0.046788</td>\n",
       "      <td>-0.040007</td>\n",
       "      <td>-0.103820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013078</td>\n",
       "      <td>-0.107090</td>\n",
       "      <td>-0.440320</td>\n",
       "      <td>-0.039817</td>\n",
       "      <td>0.092986</td>\n",
       "      <td>0.119560</td>\n",
       "      <td>0.216730</td>\n",
       "      <td>0.292790</td>\n",
       "      <td>0.199520</td>\n",
       "      <td>0.371760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.268800</td>\n",
       "      <td>-0.027697</td>\n",
       "      <td>-0.176130</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>0.273210</td>\n",
       "      <td>0.251530</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>-0.224650</td>\n",
       "      <td>-0.079456</td>\n",
       "      <td>-0.063992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107190</td>\n",
       "      <td>0.075985</td>\n",
       "      <td>-0.008486</td>\n",
       "      <td>0.081589</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>-0.295520</td>\n",
       "      <td>-0.237230</td>\n",
       "      <td>0.270260</td>\n",
       "      <td>0.266110</td>\n",
       "      <td>-0.309050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118620</td>\n",
       "      <td>-0.427520</td>\n",
       "      <td>0.046964</td>\n",
       "      <td>0.087433</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.081076</td>\n",
       "      <td>0.119380</td>\n",
       "      <td>-0.400020</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.269940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>-0.055203</td>\n",
       "      <td>0.026619</td>\n",
       "      <td>-0.240140</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>0.130810</td>\n",
       "      <td>-0.019985</td>\n",
       "      <td>-0.330250</td>\n",
       "      <td>0.159130</td>\n",
       "      <td>-0.100920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.176940</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.105950</td>\n",
       "      <td>-0.067230</td>\n",
       "      <td>-0.229590</td>\n",
       "      <td>0.411750</td>\n",
       "      <td>-0.078765</td>\n",
       "      <td>-0.151860</td>\n",
       "      <td>-0.090426</td>\n",
       "      <td>0.410520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245870</td>\n",
       "      <td>-0.182870</td>\n",
       "      <td>-0.230340</td>\n",
       "      <td>0.230250</td>\n",
       "      <td>0.619600</td>\n",
       "      <td>0.082631</td>\n",
       "      <td>-0.155040</td>\n",
       "      <td>0.051455</td>\n",
       "      <td>0.261910</td>\n",
       "      <td>-0.157840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.021599</td>\n",
       "      <td>-0.014969</td>\n",
       "      <td>0.232980</td>\n",
       "      <td>0.506390</td>\n",
       "      <td>0.120210</td>\n",
       "      <td>0.154890</td>\n",
       "      <td>0.336940</td>\n",
       "      <td>-0.048732</td>\n",
       "      <td>-0.188950</td>\n",
       "      <td>-0.092081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083593</td>\n",
       "      <td>-0.241590</td>\n",
       "      <td>0.198530</td>\n",
       "      <td>-0.017753</td>\n",
       "      <td>0.597070</td>\n",
       "      <td>0.093406</td>\n",
       "      <td>-0.303690</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>0.454480</td>\n",
       "      <td>0.297170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.090756</td>\n",
       "      <td>0.057117</td>\n",
       "      <td>0.175240</td>\n",
       "      <td>0.251990</td>\n",
       "      <td>0.219760</td>\n",
       "      <td>-0.042760</td>\n",
       "      <td>-0.394500</td>\n",
       "      <td>-0.152950</td>\n",
       "      <td>-0.091411</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238290</td>\n",
       "      <td>-0.275390</td>\n",
       "      <td>-0.081909</td>\n",
       "      <td>0.113340</td>\n",
       "      <td>0.345570</td>\n",
       "      <td>-0.045735</td>\n",
       "      <td>-0.270670</td>\n",
       "      <td>0.018656</td>\n",
       "      <td>-0.162090</td>\n",
       "      <td>0.046151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.092204</td>\n",
       "      <td>-0.090641</td>\n",
       "      <td>0.091374</td>\n",
       "      <td>-0.073267</td>\n",
       "      <td>-0.081702</td>\n",
       "      <td>-0.139190</td>\n",
       "      <td>-0.348250</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.514780</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250680</td>\n",
       "      <td>-0.361000</td>\n",
       "      <td>-0.021109</td>\n",
       "      <td>-0.120420</td>\n",
       "      <td>0.656580</td>\n",
       "      <td>-0.057016</td>\n",
       "      <td>-0.161310</td>\n",
       "      <td>0.196450</td>\n",
       "      <td>-0.184110</td>\n",
       "      <td>-0.159880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.090107</td>\n",
       "      <td>-0.112140</td>\n",
       "      <td>-0.186160</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.044789</td>\n",
       "      <td>0.493980</td>\n",
       "      <td>-0.371220</td>\n",
       "      <td>-0.091334</td>\n",
       "      <td>0.031132</td>\n",
       "      <td>-0.034730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618040</td>\n",
       "      <td>-0.155920</td>\n",
       "      <td>0.190280</td>\n",
       "      <td>-0.006354</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>-0.117730</td>\n",
       "      <td>-0.261050</td>\n",
       "      <td>0.137060</td>\n",
       "      <td>0.248140</td>\n",
       "      <td>-0.057017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.283170</td>\n",
       "      <td>-0.112300</td>\n",
       "      <td>0.039923</td>\n",
       "      <td>0.457030</td>\n",
       "      <td>-0.330840</td>\n",
       "      <td>0.445530</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>-0.215070</td>\n",
       "      <td>-0.041685</td>\n",
       "      <td>0.374010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069616</td>\n",
       "      <td>-0.077255</td>\n",
       "      <td>-0.029674</td>\n",
       "      <td>-0.396780</td>\n",
       "      <td>-0.079598</td>\n",
       "      <td>0.130320</td>\n",
       "      <td>-0.221790</td>\n",
       "      <td>0.129340</td>\n",
       "      <td>-0.232840</td>\n",
       "      <td>-0.052929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.171190</td>\n",
       "      <td>-0.171170</td>\n",
       "      <td>-0.079201</td>\n",
       "      <td>-0.066352</td>\n",
       "      <td>-0.057326</td>\n",
       "      <td>0.429540</td>\n",
       "      <td>-0.041154</td>\n",
       "      <td>-0.544420</td>\n",
       "      <td>-0.034768</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201160</td>\n",
       "      <td>-0.134660</td>\n",
       "      <td>0.198740</td>\n",
       "      <td>-0.265390</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>-0.312950</td>\n",
       "      <td>-0.121060</td>\n",
       "      <td>0.049508</td>\n",
       "      <td>0.134120</td>\n",
       "      <td>-0.090636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.306250</td>\n",
       "      <td>-0.085336</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>0.306130</td>\n",
       "      <td>-0.254020</td>\n",
       "      <td>-0.095143</td>\n",
       "      <td>0.117030</td>\n",
       "      <td>-0.303350</td>\n",
       "      <td>0.245980</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.414980</td>\n",
       "      <td>-0.018883</td>\n",
       "      <td>-0.272270</td>\n",
       "      <td>-0.221390</td>\n",
       "      <td>0.263010</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.128520</td>\n",
       "      <td>-0.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.249420</td>\n",
       "      <td>0.022730</td>\n",
       "      <td>-0.082302</td>\n",
       "      <td>-0.458330</td>\n",
       "      <td>0.380160</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>-0.150990</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>-0.236760</td>\n",
       "      <td>0.201260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056944</td>\n",
       "      <td>-0.479700</td>\n",
       "      <td>0.563360</td>\n",
       "      <td>0.081469</td>\n",
       "      <td>0.759820</td>\n",
       "      <td>-0.177290</td>\n",
       "      <td>-0.030218</td>\n",
       "      <td>-0.088029</td>\n",
       "      <td>-0.069260</td>\n",
       "      <td>-0.205820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.043751</td>\n",
       "      <td>-0.035610</td>\n",
       "      <td>0.347630</td>\n",
       "      <td>0.012232</td>\n",
       "      <td>-0.281330</td>\n",
       "      <td>0.409610</td>\n",
       "      <td>0.130420</td>\n",
       "      <td>-0.174650</td>\n",
       "      <td>0.067074</td>\n",
       "      <td>-0.105000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>-0.103530</td>\n",
       "      <td>0.323510</td>\n",
       "      <td>-0.176720</td>\n",
       "      <td>0.340930</td>\n",
       "      <td>-0.301910</td>\n",
       "      <td>-0.173790</td>\n",
       "      <td>0.434480</td>\n",
       "      <td>0.182350</td>\n",
       "      <td>0.137020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.259810</td>\n",
       "      <td>-0.628450</td>\n",
       "      <td>-0.376400</td>\n",
       "      <td>0.379380</td>\n",
       "      <td>0.697410</td>\n",
       "      <td>-0.268130</td>\n",
       "      <td>-0.055744</td>\n",
       "      <td>0.233130</td>\n",
       "      <td>-0.400970</td>\n",
       "      <td>-0.253150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127880</td>\n",
       "      <td>-0.182490</td>\n",
       "      <td>0.121010</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.181850</td>\n",
       "      <td>0.290540</td>\n",
       "      <td>0.216310</td>\n",
       "      <td>0.299860</td>\n",
       "      <td>-0.604120</td>\n",
       "      <td>-0.167670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.453180</td>\n",
       "      <td>-0.120660</td>\n",
       "      <td>-0.251000</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>-0.136780</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>0.036373</td>\n",
       "      <td>-0.551790</td>\n",
       "      <td>-0.149680</td>\n",
       "      <td>0.661930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040828</td>\n",
       "      <td>-0.007463</td>\n",
       "      <td>0.034043</td>\n",
       "      <td>-0.203840</td>\n",
       "      <td>0.235290</td>\n",
       "      <td>0.137620</td>\n",
       "      <td>0.104430</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>-0.314220</td>\n",
       "      <td>0.025882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.341610</td>\n",
       "      <td>-0.090389</td>\n",
       "      <td>-0.212770</td>\n",
       "      <td>0.257460</td>\n",
       "      <td>-0.454380</td>\n",
       "      <td>-0.234080</td>\n",
       "      <td>-0.064060</td>\n",
       "      <td>-0.749710</td>\n",
       "      <td>-0.252610</td>\n",
       "      <td>-0.014309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064772</td>\n",
       "      <td>-0.177420</td>\n",
       "      <td>0.056741</td>\n",
       "      <td>-0.119870</td>\n",
       "      <td>0.596530</td>\n",
       "      <td>-0.267840</td>\n",
       "      <td>-0.458040</td>\n",
       "      <td>0.340430</td>\n",
       "      <td>0.435510</td>\n",
       "      <td>0.191760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.009672</td>\n",
       "      <td>-0.099439</td>\n",
       "      <td>-0.007415</td>\n",
       "      <td>-0.263840</td>\n",
       "      <td>0.197660</td>\n",
       "      <td>0.225340</td>\n",
       "      <td>-0.145430</td>\n",
       "      <td>0.113330</td>\n",
       "      <td>-0.134080</td>\n",
       "      <td>0.423580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533860</td>\n",
       "      <td>-0.125180</td>\n",
       "      <td>-0.106400</td>\n",
       "      <td>-0.121390</td>\n",
       "      <td>0.769080</td>\n",
       "      <td>-0.399590</td>\n",
       "      <td>-0.315690</td>\n",
       "      <td>0.119170</td>\n",
       "      <td>0.024877</td>\n",
       "      <td>0.108940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.044589</td>\n",
       "      <td>-0.089292</td>\n",
       "      <td>0.180820</td>\n",
       "      <td>0.549540</td>\n",
       "      <td>-0.254230</td>\n",
       "      <td>-0.247600</td>\n",
       "      <td>0.280680</td>\n",
       "      <td>-0.181680</td>\n",
       "      <td>-0.255290</td>\n",
       "      <td>0.051185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002195</td>\n",
       "      <td>-0.291480</td>\n",
       "      <td>0.417880</td>\n",
       "      <td>-0.020679</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>0.456130</td>\n",
       "      <td>-0.335290</td>\n",
       "      <td>-0.050292</td>\n",
       "      <td>-0.209050</td>\n",
       "      <td>0.032764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.096480</td>\n",
       "      <td>-0.186570</td>\n",
       "      <td>-0.021864</td>\n",
       "      <td>0.267130</td>\n",
       "      <td>0.048626</td>\n",
       "      <td>0.087558</td>\n",
       "      <td>0.114240</td>\n",
       "      <td>-0.187330</td>\n",
       "      <td>0.153140</td>\n",
       "      <td>0.308210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045147</td>\n",
       "      <td>0.156130</td>\n",
       "      <td>0.542190</td>\n",
       "      <td>0.247080</td>\n",
       "      <td>-0.273080</td>\n",
       "      <td>-0.062692</td>\n",
       "      <td>-0.074539</td>\n",
       "      <td>0.226580</td>\n",
       "      <td>0.139490</td>\n",
       "      <td>-0.336650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.152460 -0.590110  0.079915  0.333420 -0.088485  0.112700 -0.114920   \n",
       "1   0.152330  0.018883  0.248440  0.308760  0.321930  0.103750 -0.057328   \n",
       "2  -0.268800 -0.027697 -0.176130  0.006205  0.273210  0.251530 -0.038377   \n",
       "3  -0.118620 -0.427520  0.046964  0.087433  0.079440  0.081076  0.119380   \n",
       "4  -0.176940  0.161100  0.105950 -0.067230 -0.229590  0.411750 -0.078765   \n",
       "5  -0.021599 -0.014969  0.232980  0.506390  0.120210  0.154890  0.336940   \n",
       "6   0.090756  0.057117  0.175240  0.251990  0.219760 -0.042760 -0.394500   \n",
       "7   0.092204 -0.090641  0.091374 -0.073267 -0.081702 -0.139190 -0.348250   \n",
       "8   0.090107 -0.112140 -0.186160  0.355300  0.044789  0.493980 -0.371220   \n",
       "9  -0.283170 -0.112300  0.039923  0.457030 -0.330840  0.445530  0.126350   \n",
       "10  0.171190 -0.171170 -0.079201 -0.066352 -0.057326  0.429540 -0.041154   \n",
       "11  0.306250 -0.085336  0.159340  0.306130 -0.254020 -0.095143  0.117030   \n",
       "12  0.249420  0.022730 -0.082302 -0.458330  0.380160  0.002072 -0.150990   \n",
       "13 -0.043751 -0.035610  0.347630  0.012232 -0.281330  0.409610  0.130420   \n",
       "14 -0.259810 -0.628450 -0.376400  0.379380  0.697410 -0.268130 -0.055744   \n",
       "15 -0.453180 -0.120660 -0.251000  0.361700 -0.136780  0.069824  0.036373   \n",
       "16  0.341610 -0.090389 -0.212770  0.257460 -0.454380 -0.234080 -0.064060   \n",
       "17 -0.009672 -0.099439 -0.007415 -0.263840  0.197660  0.225340 -0.145430   \n",
       "18  0.044589 -0.089292  0.180820  0.549540 -0.254230 -0.247600  0.280680   \n",
       "19 -0.096480 -0.186570 -0.021864  0.267130  0.048626  0.087558  0.114240   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "0  -0.579780 -0.036263  0.006680  ... -0.013480 -0.017638  0.620890 -0.043418   \n",
       "1  -0.046788 -0.040007 -0.103820  ...  0.013078 -0.107090 -0.440320 -0.039817   \n",
       "2  -0.224650 -0.079456 -0.063992  ...  0.107190  0.075985 -0.008486  0.081589   \n",
       "3  -0.400020  0.162500  0.269940  ...  0.005722 -0.055203  0.026619 -0.240140   \n",
       "4  -0.151860 -0.090426  0.410520  ...  0.245870 -0.182870 -0.230340  0.230250   \n",
       "5  -0.048732 -0.188950 -0.092081  ... -0.083593 -0.241590  0.198530 -0.017753   \n",
       "6  -0.152950 -0.091411  0.223100  ...  0.238290 -0.275390 -0.081909  0.113340   \n",
       "7  -0.232600 -0.514780  0.006573  ...  0.250680 -0.361000 -0.021109 -0.120420   \n",
       "8  -0.091334  0.031132 -0.034730  ...  0.618040 -0.155920  0.190280 -0.006354   \n",
       "9  -0.215070 -0.041685  0.374010  ...  0.069616 -0.077255 -0.029674 -0.396780   \n",
       "10 -0.544420 -0.034768  0.194140  ... -0.201160 -0.134660  0.198740 -0.265390   \n",
       "11 -0.303350  0.245980 -0.016352  ... -0.062463  0.140600  0.414980 -0.018883   \n",
       "12  0.032540 -0.236760  0.201260  ... -0.056944 -0.479700  0.563360  0.081469   \n",
       "13 -0.174650  0.067074 -0.105000  ...  0.505200 -0.103530  0.323510 -0.176720   \n",
       "14  0.233130 -0.400970 -0.253150  ...  0.127880 -0.182490  0.121010  0.020481   \n",
       "15 -0.551790 -0.149680  0.661930  ... -0.040828 -0.007463  0.034043 -0.203840   \n",
       "16 -0.749710 -0.252610 -0.014309  ...  0.064772 -0.177420  0.056741 -0.119870   \n",
       "17  0.113330 -0.134080  0.423580  ...  0.533860 -0.125180 -0.106400 -0.121390   \n",
       "18 -0.181680 -0.255290  0.051185  ... -0.002195 -0.291480  0.417880 -0.020679   \n",
       "19 -0.187330  0.153140  0.308210  ...  0.045147  0.156130  0.542190  0.247080   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "0   0.052670  0.565990 -0.065357 -0.507710  0.120370  0.155420  \n",
       "1   0.092986  0.119560  0.216730  0.292790  0.199520  0.371760  \n",
       "2   0.020419 -0.295520 -0.237230  0.270260  0.266110 -0.309050  \n",
       "3   0.164890  0.130810 -0.019985 -0.330250  0.159130 -0.100920  \n",
       "4   0.619600  0.082631 -0.155040  0.051455  0.261910 -0.157840  \n",
       "5   0.597070  0.093406 -0.303690  0.127090  0.454480  0.297170  \n",
       "6   0.345570 -0.045735 -0.270670  0.018656 -0.162090  0.046151  \n",
       "7   0.656580 -0.057016 -0.161310  0.196450 -0.184110 -0.159880  \n",
       "8   0.017190 -0.117730 -0.261050  0.137060  0.248140 -0.057017  \n",
       "9  -0.079598  0.130320 -0.221790  0.129340 -0.232840 -0.052929  \n",
       "10  0.393200 -0.312950 -0.121060  0.049508  0.134120 -0.090636  \n",
       "11 -0.272270 -0.221390  0.263010  0.001109  0.128520 -0.282400  \n",
       "12  0.759820 -0.177290 -0.030218 -0.088029 -0.069260 -0.205820  \n",
       "13  0.340930 -0.301910 -0.173790  0.434480  0.182350  0.137020  \n",
       "14  0.181850  0.290540  0.216310  0.299860 -0.604120 -0.167670  \n",
       "15  0.235290  0.137620  0.104430  0.092619 -0.314220  0.025882  \n",
       "16  0.596530 -0.267840 -0.458040  0.340430  0.435510  0.191760  \n",
       "17  0.769080 -0.399590 -0.315690  0.119170  0.024877  0.108940  \n",
       "18  0.006406  0.456130 -0.335290 -0.050292 -0.209050  0.032764  \n",
       "19 -0.273080 -0.062692 -0.074539  0.226580  0.139490 -0.336650  \n",
       "\n",
       "[20 rows x 300 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings\n",
    "all_df = pd.concat([all_df, topics])\n",
    "\n",
    "# Add topic labels to overall label list\n",
    "labels = np.concatenate((data['label'], range(20)), axis=None) \n",
    "\n",
    "# Create indicators whether a row is a topic or a document\n",
    "is_topic = [0 if i<len(data['text']) else 1 for i in range(len(all_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_df) == len(labels) == len(is_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "        'mideast', 'space', 'sale', 'politics', 'graphics',\n",
    "        'cryptography', 'windows', 'microsoft', 'guns',\n",
    "        'religion', 'autos', 'medicine', 'mac', 'electronics',\n",
    "        'hockey', 'atheism', 'motorcycles', 'pc', 'baseball', 'christian'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_df\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "             X, y, test_size = 0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n",
      "/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "        'mideast', 'space', 'sale', 'politics', 'graphics',\n",
    "        'cryptography', 'windows', 'microsoft', 'guns',\n",
    "        'religion', 'autos', 'medicine', 'mac', 'electronics',\n",
    "        'hockey', 'atheism', 'motorcycles', 'pc', 'baseball', 'christian'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mideast       0.81      0.89      0.84       174\n",
      "       space       0.68      0.75      0.71       199\n",
      "        sale       0.70      0.76      0.73       189\n",
      "    politics       0.59      0.67      0.63       162\n",
      "    graphics       0.49      0.57      0.53       194\n",
      "cryptography       0.61      0.87      0.72       211\n",
      "     windows       0.63      0.59      0.61       206\n",
      "   microsoft       0.64      0.57      0.61       222\n",
      "        guns       0.62      0.74      0.67       183\n",
      "    religion       0.47      0.43      0.45       126\n",
      "       autos       0.77      0.67      0.72       208\n",
      "    medicine       0.90      0.75      0.82       194\n",
      "         mac       0.65      0.49      0.56       191\n",
      " electronics       0.71      0.57      0.63       198\n",
      "      hockey       0.93      0.92      0.93       194\n",
      "     atheism       0.64      0.50      0.56       159\n",
      " motorcycles       0.71      0.73      0.72       195\n",
      "          pc       0.51      0.49      0.50       189\n",
      "    baseball       0.90      0.85      0.87       184\n",
      "   christian       0.66      0.67      0.66       188\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3766\n",
      "   macro avg       0.68      0.67      0.67      3766\n",
      "weighted avg       0.68      0.68      0.68      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67870419543282\n"
     ]
    }
   ],
   "source": [
    "print(classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_classes(distances):\n",
    "    \"\"\"\n",
    "    Iteratively assigns classes to clusters\n",
    "    \n",
    "    Args:\n",
    "        distances (np array): embed_dim x embed_dim\n",
    "        \n",
    "    Returns:\n",
    "        dict {centroid #: class #}\n",
    "    \"\"\"\n",
    "    class_map = {}\n",
    "    n = len(distances)\n",
    "    clusters = list(range(n))\n",
    "    classes = list(range(n))\n",
    "    \n",
    "    while n > 0:\n",
    "        # Find new least distance pair\n",
    "        ind = np.argmin(distances)\n",
    "        \n",
    "        # Get cluster, class index in the matrix\n",
    "        class_i = int(ind % n)\n",
    "        cluster_i = ind // n\n",
    "        \n",
    "        # Enter the actual values\n",
    "        class_map[clusters[cluster_i]] = classes[class_i] \n",
    "        \n",
    "        # update n, distances\n",
    "        distances = np.delete(distances, (cluster_i), axis=0) # delete row\n",
    "        distances = np.delete(distances, (class_i), axis=1) # delete column\n",
    "        clusters.remove(clusters[cluster_i])\n",
    "        classes.remove(classes[class_i])\n",
    "        n -= 1\n",
    "        \n",
    "    return class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_kmeans(data, n_clusters, labels, topic_embeds):\n",
    "    \"\"\"\n",
    "    Learns a kmeans and matches the learned clusters to classes.\n",
    "    Computes the training accuracy.\n",
    "    \n",
    "    Args:\n",
    "        data (df): df of document embedding vectors\n",
    "        n_clusters (int): number of clusters we want for the kmeans learning\n",
    "        labels (set): set of labels referring to classes to match to\n",
    "        topic_embeds (df): topic embeddings df\n",
    "        \n",
    "    Returns:\n",
    "        training accuracy\n",
    "    \"\"\"\n",
    "    # kmeans cluster\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n",
    "    \n",
    "    # kmeans predictions\n",
    "    preds = kmeans.labels_\n",
    "    \n",
    "    # get centroids of learned clusters\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # get topic embeddings\n",
    "    topics = np.array(topic_embeds.iloc[list(set(labels))])\n",
    "    \n",
    "    # compute squared distances between centroids and topics\n",
    "    distances = np.dot(centroids, topics.T) # output: n_clusters x n_clusters \n",
    "    \n",
    "    # assign classes to clusters\n",
    "    class_map = assign_classes(distances) # {class index: topic index}\n",
    "    class_preds = [class_map[pred] for pred in preds]\n",
    "    \n",
    "    # compute accuracy between labels, preds\n",
    "    return sum(1 for x,y in zip(class_preds, labels) if x == y) / len(class_preds), centroids, class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, centroids, class_map = learn_kmeans(te_df, 7, test_data['label'], topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.1861816973924597\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA all embeddings, then add other labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two components for visualization\n",
    "pca = PCA(n_components=2).fit(te_df)\n",
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_pca = pd.DataFrame(pca.transform(centroids))\n",
    "centroid_pca['label'] = [class_map[i] for i in range(len(centroid_pca))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142586</td>\n",
       "      <td>-0.263593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.348948</td>\n",
       "      <td>-1.139837</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.205995</td>\n",
       "      <td>0.117844</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011818</td>\n",
       "      <td>-0.427489</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.237216</td>\n",
       "      <td>0.406908</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.107764</td>\n",
       "      <td>0.669438</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.646321</td>\n",
       "      <td>-0.094454</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  label\n",
       "0  0.142586 -0.263593      0\n",
       "1 -0.348948 -1.139837      1\n",
       "2 -0.205995  0.117844      2\n",
       "3 -0.011818 -0.427489      3\n",
       "4 -0.237216  0.406908      4\n",
       "5 -0.107764  0.669438      5\n",
       "6 -0.646321 -0.094454      6"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_pca = pd.DataFrame(pca.transform(topics.iloc[list(set(test_data['label']))]))\n",
    "topics_pca['label'] = range(len(topics_pca))\n",
    "\n",
    "topics_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PCA(2) Cluster Centroids and Corresponding Labels')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "centroid_pca.plot.scatter(x=0, y=1,\n",
    "                  c='label',\n",
    "                  s=30,\n",
    "                  colormap='viridis')\n",
    "plt.scatter(topics_pca[0], topics_pca[1], s=50, marker='+', c=topics_pca['label'])\n",
    "for i, row in topics_pca.iterrows():\n",
    "    plt.text(row[0]+0.05, row[1]+0.07, classes[int(row['label'])], fontsize=9)\n",
    "plt.title('PCA(2) Cluster Centroids and Corresponding Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
