{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import signal\n",
    "import argparse\n",
    "import traceback\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0323 11:48:47.632786 140472081618752 file_utils.py:39] PyTorch version 1.3.1 available.\n"
     ]
    }
   ],
   "source": [
    "# Project libraries\n",
    "sys.path.insert(1, '../src/')\n",
    "\n",
    "import embedding.factory as ebd\n",
    "import classifier.factory as clf\n",
    "import dataset.loader as loader\n",
    "import train.factory as train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': '../data/20news.json', 'dataset': '20newsgroup', 'n_train_class': 8, 'n_val_class': 5, 'n_test_class': 7, 'mode': 'test', 'wv_path': '../', 'word_vector': 'wiki.en.vec', 'finetune_ebd': True, 'bert': False, 'auxiliary': [], 'embedding': 'avg', 'meta_w_target': False, 'cuda': 0, 'snapshot': ''}\n"
     ]
    }
   ],
   "source": [
    "# Build args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_path\", type=str,\n",
    "                        default=\"data/20news.json\", # og: reuters\n",
    "                        help=\"path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"20newsgroup\", # og: reuters\n",
    "                    help=\"name of the dataset. \"\n",
    "                    \"Options: [20newsgroup, amazon, huffpost, \"\n",
    "                    \"reuters, rcv1, fewrel]\")\n",
    "parser.add_argument(\"--n_train_class\", type=int, default=15,\n",
    "                    help=\"number of meta-train classes\")\n",
    "parser.add_argument(\"--n_val_class\", type=int, default=5,\n",
    "                    help=\"number of meta-val classes\")\n",
    "parser.add_argument(\"--n_test_class\", type=int, default=11,\n",
    "                    help=\"number of meta-test classes\")\n",
    "parser.add_argument(\"--mode\", type=str, default=\"test\",\n",
    "                    help=(\"Running mode.\"\n",
    "                          \"Options: [train, test, finetune]\"\n",
    "                          \"[Default: test]\"))\n",
    "parser.add_argument(\"--wv_path\", type=str,\n",
    "                    default=\"./\",\n",
    "                    help=\"path to word vector cache\")\n",
    "parser.add_argument(\"--word_vector\", type=str, default=\"wiki.en.vec\",\n",
    "                    help=(\"Name of pretrained word embeddings.\"))\n",
    "parser.add_argument(\"--finetune_ebd\", action=\"store_true\", default=False,\n",
    "                    help=(\"Finetune embedding during meta-training\"))\n",
    "parser.add_argument(\"--bert\", default=False, action=\"store_true\",\n",
    "                    help=(\"set true if use bert embeddings \"\n",
    "                          \"(only available for sent-level datasets: \"\n",
    "                          \"huffpost, fewrel\"))\n",
    "parser.add_argument(\"--auxiliary\", type=str, nargs=\"*\", default=[],\n",
    "                    help=(\"auxiliary embeddings (used for fewrel). \"\n",
    "                          \"Options: [pos, ent]\"))\n",
    "parser.add_argument(\"--embedding\", type=str, default=\"avg\",\n",
    "                    help=(\"document embedding method. Options: \"\n",
    "                          \"[avg, tfidf, meta, oracle, cnn]\"))\n",
    "parser.add_argument(\"--meta_w_target\", action=\"store_true\", default=False,\n",
    "                    help=\"use target importance score\")\n",
    "parser.add_argument(\"--cuda\", type=int, default=-1,\n",
    "                    help=\"cuda device, -1 for cpu\")\n",
    "parser.add_argument(\"--snapshot\", type=str, default=\"\",\n",
    "                    help=\"path to the pretraiend weights\")\n",
    "\n",
    "\n",
    "# Populate parameters\n",
    "args = parser.parse_args([\"--data_path\", \"../data/20news.json\",\n",
    "                            \"--dataset\", \"20newsgroup\",\n",
    "                            \"--n_train_class\", \"8\",\n",
    "                            \"--n_val_class\", \"5\",\n",
    "                            \"--n_test_class\", \"7\",\n",
    "                            \"--wv_path\", \"../\",\n",
    "                            \"--cuda\", \"0\",\n",
    "                            \"--finetune\"\n",
    "                            ])\n",
    "\n",
    "print(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/23 13:37:47: Loading data from ../data/20news.json\n",
      "20/03/23 13:37:48: Class balance:\n",
      "{0: 940, 1: 987, 2: 972, 3: 775, 4: 973, 5: 991, 6: 980, 7: 985, 8: 910, 9: 628, 10: 990, 11: 990, 12: 961, 13: 981, 14: 999, 15: 799, 16: 994, 17: 982, 18: 994, 19: 997}\n",
      "20/03/23 13:37:48: Avg len: 340.86015508816655\n",
      "20/03/23 13:37:48: Loading word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0323 13:37:48.817018 140472081618752 vocab.py:431] Loading vectors from ../wiki.en.vec.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/23 13:37:53: Total num. of words: 32137, word vector dimension: 300\n",
      "20/03/23 13:37:53: Num. of out-of-vocabulary words(they are initialized to zeros): 9095\n",
      "20/03/23 13:37:53: #train 7926, #val 4881, #test 6021\n"
     ]
    }
   ],
   "source": [
    "train_data, _, test_data, vocab = loader.load_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 13947 documents, 19 different labels and 32137 size vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Aggregate all training, val, test data\n",
    "data = {}\n",
    "\n",
    "for key in test_data:\n",
    "    if key == 'vocab_size':\n",
    "        data['vocab_size'] = test_data[key]\n",
    "        continue\n",
    "        \n",
    "    ax = 0 if key == 'text' else None\n",
    "    concat = np.concatenate(\n",
    "            (test_data[key], train_data[key]), axis=ax\n",
    "        )\n",
    "    \n",
    "    if key == 'text':\n",
    "        data[key] = torch.tensor(concat, device=cuda0)\n",
    "    else:\n",
    "        data[key] = concat\n",
    "\n",
    "assert len(data['text']) == len(test_data['text']) + len(train_data['text'])\n",
    "\n",
    "print('Dataset has {} documents, {} different labels and {} size vocabulary.'.format(\n",
    "    len(data['text']), max(data['label']), data['vocab_size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/23 14:44:03, Building embedding\n",
      "Embedding type: WORDEBD\n",
      "Using:  avg\n",
      "20/03/23 14:44:03, Building embedding\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "model[\"ebd\"] = ebd.get_embedding(vocab, args)\n",
    "embed = model['ebd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 7.79 GiB (GPU 0; 11.93 GiB total capacity; 8.03 GiB already allocated; 2.45 GiB free; 23.57 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-eda6faf7f940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/scratch/emcheng/project/src/embedding/avg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, weights)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         '''\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mebd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mebd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/scratch/emcheng/project/src/embedding/wordebd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, weights)\u001b[0m\n\u001b[1;32m     29\u001b[0m         '''\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune_ebd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rsg/nlp/rmwu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 7.79 GiB (GPU 0; 11.93 GiB total capacity; 8.03 GiB already allocated; 2.45 GiB free; 23.57 MiB cached)"
     ]
    }
   ],
   "source": [
    "# TODO: chunk data into sets of 100 and embed\n",
    "embed(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Topic Embeddings to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../20news_reps_cache_.json') as json_file:\n",
    "                topics = json.load(json_file) \n",
    "topics = pd.DataFrame.from_dict(topics, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.152460</td>\n",
       "      <td>-0.590110</td>\n",
       "      <td>0.079915</td>\n",
       "      <td>0.333420</td>\n",
       "      <td>-0.088485</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>-0.114920</td>\n",
       "      <td>-0.579780</td>\n",
       "      <td>-0.036263</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013480</td>\n",
       "      <td>-0.017638</td>\n",
       "      <td>0.620890</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>0.052670</td>\n",
       "      <td>0.565990</td>\n",
       "      <td>-0.065357</td>\n",
       "      <td>-0.507710</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.155420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.152330</td>\n",
       "      <td>0.018883</td>\n",
       "      <td>0.248440</td>\n",
       "      <td>0.308760</td>\n",
       "      <td>0.321930</td>\n",
       "      <td>0.103750</td>\n",
       "      <td>-0.057328</td>\n",
       "      <td>-0.046788</td>\n",
       "      <td>-0.040007</td>\n",
       "      <td>-0.103820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013078</td>\n",
       "      <td>-0.107090</td>\n",
       "      <td>-0.440320</td>\n",
       "      <td>-0.039817</td>\n",
       "      <td>0.092986</td>\n",
       "      <td>0.119560</td>\n",
       "      <td>0.216730</td>\n",
       "      <td>0.292790</td>\n",
       "      <td>0.199520</td>\n",
       "      <td>0.371760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.268800</td>\n",
       "      <td>-0.027697</td>\n",
       "      <td>-0.176130</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>0.273210</td>\n",
       "      <td>0.251530</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>-0.224650</td>\n",
       "      <td>-0.079456</td>\n",
       "      <td>-0.063992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107190</td>\n",
       "      <td>0.075985</td>\n",
       "      <td>-0.008486</td>\n",
       "      <td>0.081589</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>-0.295520</td>\n",
       "      <td>-0.237230</td>\n",
       "      <td>0.270260</td>\n",
       "      <td>0.266110</td>\n",
       "      <td>-0.309050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.118620</td>\n",
       "      <td>-0.427520</td>\n",
       "      <td>0.046964</td>\n",
       "      <td>0.087433</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.081076</td>\n",
       "      <td>0.119380</td>\n",
       "      <td>-0.400020</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.269940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>-0.055203</td>\n",
       "      <td>0.026619</td>\n",
       "      <td>-0.240140</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>0.130810</td>\n",
       "      <td>-0.019985</td>\n",
       "      <td>-0.330250</td>\n",
       "      <td>0.159130</td>\n",
       "      <td>-0.100920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.176940</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.105950</td>\n",
       "      <td>-0.067230</td>\n",
       "      <td>-0.229590</td>\n",
       "      <td>0.411750</td>\n",
       "      <td>-0.078765</td>\n",
       "      <td>-0.151860</td>\n",
       "      <td>-0.090426</td>\n",
       "      <td>0.410520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245870</td>\n",
       "      <td>-0.182870</td>\n",
       "      <td>-0.230340</td>\n",
       "      <td>0.230250</td>\n",
       "      <td>0.619600</td>\n",
       "      <td>0.082631</td>\n",
       "      <td>-0.155040</td>\n",
       "      <td>0.051455</td>\n",
       "      <td>0.261910</td>\n",
       "      <td>-0.157840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.021599</td>\n",
       "      <td>-0.014969</td>\n",
       "      <td>0.232980</td>\n",
       "      <td>0.506390</td>\n",
       "      <td>0.120210</td>\n",
       "      <td>0.154890</td>\n",
       "      <td>0.336940</td>\n",
       "      <td>-0.048732</td>\n",
       "      <td>-0.188950</td>\n",
       "      <td>-0.092081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083593</td>\n",
       "      <td>-0.241590</td>\n",
       "      <td>0.198530</td>\n",
       "      <td>-0.017753</td>\n",
       "      <td>0.597070</td>\n",
       "      <td>0.093406</td>\n",
       "      <td>-0.303690</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>0.454480</td>\n",
       "      <td>0.297170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.090756</td>\n",
       "      <td>0.057117</td>\n",
       "      <td>0.175240</td>\n",
       "      <td>0.251990</td>\n",
       "      <td>0.219760</td>\n",
       "      <td>-0.042760</td>\n",
       "      <td>-0.394500</td>\n",
       "      <td>-0.152950</td>\n",
       "      <td>-0.091411</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238290</td>\n",
       "      <td>-0.275390</td>\n",
       "      <td>-0.081909</td>\n",
       "      <td>0.113340</td>\n",
       "      <td>0.345570</td>\n",
       "      <td>-0.045735</td>\n",
       "      <td>-0.270670</td>\n",
       "      <td>0.018656</td>\n",
       "      <td>-0.162090</td>\n",
       "      <td>0.046151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.092204</td>\n",
       "      <td>-0.090641</td>\n",
       "      <td>0.091374</td>\n",
       "      <td>-0.073267</td>\n",
       "      <td>-0.081702</td>\n",
       "      <td>-0.139190</td>\n",
       "      <td>-0.348250</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.514780</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250680</td>\n",
       "      <td>-0.361000</td>\n",
       "      <td>-0.021109</td>\n",
       "      <td>-0.120420</td>\n",
       "      <td>0.656580</td>\n",
       "      <td>-0.057016</td>\n",
       "      <td>-0.161310</td>\n",
       "      <td>0.196450</td>\n",
       "      <td>-0.184110</td>\n",
       "      <td>-0.159880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.090107</td>\n",
       "      <td>-0.112140</td>\n",
       "      <td>-0.186160</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.044789</td>\n",
       "      <td>0.493980</td>\n",
       "      <td>-0.371220</td>\n",
       "      <td>-0.091334</td>\n",
       "      <td>0.031132</td>\n",
       "      <td>-0.034730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618040</td>\n",
       "      <td>-0.155920</td>\n",
       "      <td>0.190280</td>\n",
       "      <td>-0.006354</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>-0.117730</td>\n",
       "      <td>-0.261050</td>\n",
       "      <td>0.137060</td>\n",
       "      <td>0.248140</td>\n",
       "      <td>-0.057017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.283170</td>\n",
       "      <td>-0.112300</td>\n",
       "      <td>0.039923</td>\n",
       "      <td>0.457030</td>\n",
       "      <td>-0.330840</td>\n",
       "      <td>0.445530</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>-0.215070</td>\n",
       "      <td>-0.041685</td>\n",
       "      <td>0.374010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069616</td>\n",
       "      <td>-0.077255</td>\n",
       "      <td>-0.029674</td>\n",
       "      <td>-0.396780</td>\n",
       "      <td>-0.079598</td>\n",
       "      <td>0.130320</td>\n",
       "      <td>-0.221790</td>\n",
       "      <td>0.129340</td>\n",
       "      <td>-0.232840</td>\n",
       "      <td>-0.052929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.171190</td>\n",
       "      <td>-0.171170</td>\n",
       "      <td>-0.079201</td>\n",
       "      <td>-0.066352</td>\n",
       "      <td>-0.057326</td>\n",
       "      <td>0.429540</td>\n",
       "      <td>-0.041154</td>\n",
       "      <td>-0.544420</td>\n",
       "      <td>-0.034768</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201160</td>\n",
       "      <td>-0.134660</td>\n",
       "      <td>0.198740</td>\n",
       "      <td>-0.265390</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>-0.312950</td>\n",
       "      <td>-0.121060</td>\n",
       "      <td>0.049508</td>\n",
       "      <td>0.134120</td>\n",
       "      <td>-0.090636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.306250</td>\n",
       "      <td>-0.085336</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>0.306130</td>\n",
       "      <td>-0.254020</td>\n",
       "      <td>-0.095143</td>\n",
       "      <td>0.117030</td>\n",
       "      <td>-0.303350</td>\n",
       "      <td>0.245980</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.414980</td>\n",
       "      <td>-0.018883</td>\n",
       "      <td>-0.272270</td>\n",
       "      <td>-0.221390</td>\n",
       "      <td>0.263010</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.128520</td>\n",
       "      <td>-0.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.249420</td>\n",
       "      <td>0.022730</td>\n",
       "      <td>-0.082302</td>\n",
       "      <td>-0.458330</td>\n",
       "      <td>0.380160</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>-0.150990</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>-0.236760</td>\n",
       "      <td>0.201260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056944</td>\n",
       "      <td>-0.479700</td>\n",
       "      <td>0.563360</td>\n",
       "      <td>0.081469</td>\n",
       "      <td>0.759820</td>\n",
       "      <td>-0.177290</td>\n",
       "      <td>-0.030218</td>\n",
       "      <td>-0.088029</td>\n",
       "      <td>-0.069260</td>\n",
       "      <td>-0.205820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.043751</td>\n",
       "      <td>-0.035610</td>\n",
       "      <td>0.347630</td>\n",
       "      <td>0.012232</td>\n",
       "      <td>-0.281330</td>\n",
       "      <td>0.409610</td>\n",
       "      <td>0.130420</td>\n",
       "      <td>-0.174650</td>\n",
       "      <td>0.067074</td>\n",
       "      <td>-0.105000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>-0.103530</td>\n",
       "      <td>0.323510</td>\n",
       "      <td>-0.176720</td>\n",
       "      <td>0.340930</td>\n",
       "      <td>-0.301910</td>\n",
       "      <td>-0.173790</td>\n",
       "      <td>0.434480</td>\n",
       "      <td>0.182350</td>\n",
       "      <td>0.137020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.259810</td>\n",
       "      <td>-0.628450</td>\n",
       "      <td>-0.376400</td>\n",
       "      <td>0.379380</td>\n",
       "      <td>0.697410</td>\n",
       "      <td>-0.268130</td>\n",
       "      <td>-0.055744</td>\n",
       "      <td>0.233130</td>\n",
       "      <td>-0.400970</td>\n",
       "      <td>-0.253150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127880</td>\n",
       "      <td>-0.182490</td>\n",
       "      <td>0.121010</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.181850</td>\n",
       "      <td>0.290540</td>\n",
       "      <td>0.216310</td>\n",
       "      <td>0.299860</td>\n",
       "      <td>-0.604120</td>\n",
       "      <td>-0.167670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.453180</td>\n",
       "      <td>-0.120660</td>\n",
       "      <td>-0.251000</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>-0.136780</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>0.036373</td>\n",
       "      <td>-0.551790</td>\n",
       "      <td>-0.149680</td>\n",
       "      <td>0.661930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040828</td>\n",
       "      <td>-0.007463</td>\n",
       "      <td>0.034043</td>\n",
       "      <td>-0.203840</td>\n",
       "      <td>0.235290</td>\n",
       "      <td>0.137620</td>\n",
       "      <td>0.104430</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>-0.314220</td>\n",
       "      <td>0.025882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.341610</td>\n",
       "      <td>-0.090389</td>\n",
       "      <td>-0.212770</td>\n",
       "      <td>0.257460</td>\n",
       "      <td>-0.454380</td>\n",
       "      <td>-0.234080</td>\n",
       "      <td>-0.064060</td>\n",
       "      <td>-0.749710</td>\n",
       "      <td>-0.252610</td>\n",
       "      <td>-0.014309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064772</td>\n",
       "      <td>-0.177420</td>\n",
       "      <td>0.056741</td>\n",
       "      <td>-0.119870</td>\n",
       "      <td>0.596530</td>\n",
       "      <td>-0.267840</td>\n",
       "      <td>-0.458040</td>\n",
       "      <td>0.340430</td>\n",
       "      <td>0.435510</td>\n",
       "      <td>0.191760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.009672</td>\n",
       "      <td>-0.099439</td>\n",
       "      <td>-0.007415</td>\n",
       "      <td>-0.263840</td>\n",
       "      <td>0.197660</td>\n",
       "      <td>0.225340</td>\n",
       "      <td>-0.145430</td>\n",
       "      <td>0.113330</td>\n",
       "      <td>-0.134080</td>\n",
       "      <td>0.423580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533860</td>\n",
       "      <td>-0.125180</td>\n",
       "      <td>-0.106400</td>\n",
       "      <td>-0.121390</td>\n",
       "      <td>0.769080</td>\n",
       "      <td>-0.399590</td>\n",
       "      <td>-0.315690</td>\n",
       "      <td>0.119170</td>\n",
       "      <td>0.024877</td>\n",
       "      <td>0.108940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.044589</td>\n",
       "      <td>-0.089292</td>\n",
       "      <td>0.180820</td>\n",
       "      <td>0.549540</td>\n",
       "      <td>-0.254230</td>\n",
       "      <td>-0.247600</td>\n",
       "      <td>0.280680</td>\n",
       "      <td>-0.181680</td>\n",
       "      <td>-0.255290</td>\n",
       "      <td>0.051185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002195</td>\n",
       "      <td>-0.291480</td>\n",
       "      <td>0.417880</td>\n",
       "      <td>-0.020679</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>0.456130</td>\n",
       "      <td>-0.335290</td>\n",
       "      <td>-0.050292</td>\n",
       "      <td>-0.209050</td>\n",
       "      <td>0.032764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.096480</td>\n",
       "      <td>-0.186570</td>\n",
       "      <td>-0.021864</td>\n",
       "      <td>0.267130</td>\n",
       "      <td>0.048626</td>\n",
       "      <td>0.087558</td>\n",
       "      <td>0.114240</td>\n",
       "      <td>-0.187330</td>\n",
       "      <td>0.153140</td>\n",
       "      <td>0.308210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045147</td>\n",
       "      <td>0.156130</td>\n",
       "      <td>0.542190</td>\n",
       "      <td>0.247080</td>\n",
       "      <td>-0.273080</td>\n",
       "      <td>-0.062692</td>\n",
       "      <td>-0.074539</td>\n",
       "      <td>0.226580</td>\n",
       "      <td>0.139490</td>\n",
       "      <td>-0.336650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.152460 -0.590110  0.079915  0.333420 -0.088485  0.112700 -0.114920   \n",
       "1   0.152330  0.018883  0.248440  0.308760  0.321930  0.103750 -0.057328   \n",
       "2  -0.268800 -0.027697 -0.176130  0.006205  0.273210  0.251530 -0.038377   \n",
       "3  -0.118620 -0.427520  0.046964  0.087433  0.079440  0.081076  0.119380   \n",
       "4  -0.176940  0.161100  0.105950 -0.067230 -0.229590  0.411750 -0.078765   \n",
       "5  -0.021599 -0.014969  0.232980  0.506390  0.120210  0.154890  0.336940   \n",
       "6   0.090756  0.057117  0.175240  0.251990  0.219760 -0.042760 -0.394500   \n",
       "7   0.092204 -0.090641  0.091374 -0.073267 -0.081702 -0.139190 -0.348250   \n",
       "8   0.090107 -0.112140 -0.186160  0.355300  0.044789  0.493980 -0.371220   \n",
       "9  -0.283170 -0.112300  0.039923  0.457030 -0.330840  0.445530  0.126350   \n",
       "10  0.171190 -0.171170 -0.079201 -0.066352 -0.057326  0.429540 -0.041154   \n",
       "11  0.306250 -0.085336  0.159340  0.306130 -0.254020 -0.095143  0.117030   \n",
       "12  0.249420  0.022730 -0.082302 -0.458330  0.380160  0.002072 -0.150990   \n",
       "13 -0.043751 -0.035610  0.347630  0.012232 -0.281330  0.409610  0.130420   \n",
       "14 -0.259810 -0.628450 -0.376400  0.379380  0.697410 -0.268130 -0.055744   \n",
       "15 -0.453180 -0.120660 -0.251000  0.361700 -0.136780  0.069824  0.036373   \n",
       "16  0.341610 -0.090389 -0.212770  0.257460 -0.454380 -0.234080 -0.064060   \n",
       "17 -0.009672 -0.099439 -0.007415 -0.263840  0.197660  0.225340 -0.145430   \n",
       "18  0.044589 -0.089292  0.180820  0.549540 -0.254230 -0.247600  0.280680   \n",
       "19 -0.096480 -0.186570 -0.021864  0.267130  0.048626  0.087558  0.114240   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "0  -0.579780 -0.036263  0.006680  ... -0.013480 -0.017638  0.620890 -0.043418   \n",
       "1  -0.046788 -0.040007 -0.103820  ...  0.013078 -0.107090 -0.440320 -0.039817   \n",
       "2  -0.224650 -0.079456 -0.063992  ...  0.107190  0.075985 -0.008486  0.081589   \n",
       "3  -0.400020  0.162500  0.269940  ...  0.005722 -0.055203  0.026619 -0.240140   \n",
       "4  -0.151860 -0.090426  0.410520  ...  0.245870 -0.182870 -0.230340  0.230250   \n",
       "5  -0.048732 -0.188950 -0.092081  ... -0.083593 -0.241590  0.198530 -0.017753   \n",
       "6  -0.152950 -0.091411  0.223100  ...  0.238290 -0.275390 -0.081909  0.113340   \n",
       "7  -0.232600 -0.514780  0.006573  ...  0.250680 -0.361000 -0.021109 -0.120420   \n",
       "8  -0.091334  0.031132 -0.034730  ...  0.618040 -0.155920  0.190280 -0.006354   \n",
       "9  -0.215070 -0.041685  0.374010  ...  0.069616 -0.077255 -0.029674 -0.396780   \n",
       "10 -0.544420 -0.034768  0.194140  ... -0.201160 -0.134660  0.198740 -0.265390   \n",
       "11 -0.303350  0.245980 -0.016352  ... -0.062463  0.140600  0.414980 -0.018883   \n",
       "12  0.032540 -0.236760  0.201260  ... -0.056944 -0.479700  0.563360  0.081469   \n",
       "13 -0.174650  0.067074 -0.105000  ...  0.505200 -0.103530  0.323510 -0.176720   \n",
       "14  0.233130 -0.400970 -0.253150  ...  0.127880 -0.182490  0.121010  0.020481   \n",
       "15 -0.551790 -0.149680  0.661930  ... -0.040828 -0.007463  0.034043 -0.203840   \n",
       "16 -0.749710 -0.252610 -0.014309  ...  0.064772 -0.177420  0.056741 -0.119870   \n",
       "17  0.113330 -0.134080  0.423580  ...  0.533860 -0.125180 -0.106400 -0.121390   \n",
       "18 -0.181680 -0.255290  0.051185  ... -0.002195 -0.291480  0.417880 -0.020679   \n",
       "19 -0.187330  0.153140  0.308210  ...  0.045147  0.156130  0.542190  0.247080   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "0   0.052670  0.565990 -0.065357 -0.507710  0.120370  0.155420  \n",
       "1   0.092986  0.119560  0.216730  0.292790  0.199520  0.371760  \n",
       "2   0.020419 -0.295520 -0.237230  0.270260  0.266110 -0.309050  \n",
       "3   0.164890  0.130810 -0.019985 -0.330250  0.159130 -0.100920  \n",
       "4   0.619600  0.082631 -0.155040  0.051455  0.261910 -0.157840  \n",
       "5   0.597070  0.093406 -0.303690  0.127090  0.454480  0.297170  \n",
       "6   0.345570 -0.045735 -0.270670  0.018656 -0.162090  0.046151  \n",
       "7   0.656580 -0.057016 -0.161310  0.196450 -0.184110 -0.159880  \n",
       "8   0.017190 -0.117730 -0.261050  0.137060  0.248140 -0.057017  \n",
       "9  -0.079598  0.130320 -0.221790  0.129340 -0.232840 -0.052929  \n",
       "10  0.393200 -0.312950 -0.121060  0.049508  0.134120 -0.090636  \n",
       "11 -0.272270 -0.221390  0.263010  0.001109  0.128520 -0.282400  \n",
       "12  0.759820 -0.177290 -0.030218 -0.088029 -0.069260 -0.205820  \n",
       "13  0.340930 -0.301910 -0.173790  0.434480  0.182350  0.137020  \n",
       "14  0.181850  0.290540  0.216310  0.299860 -0.604120 -0.167670  \n",
       "15  0.235290  0.137620  0.104430  0.092619 -0.314220  0.025882  \n",
       "16  0.596530 -0.267840 -0.458040  0.340430  0.435510  0.191760  \n",
       "17  0.769080 -0.399590 -0.315690  0.119170  0.024877  0.108940  \n",
       "18  0.006406  0.456130 -0.335290 -0.050292 -0.209050  0.032764  \n",
       "19 -0.273080 -0.062692 -0.074539  0.226580  0.139490 -0.336650  \n",
       "\n",
       "[20 rows x 300 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add doc embeddings to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2 components for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pd.DataFrame(pca.fit_transform(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.883725</td>\n",
       "      <td>0.541541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.305743</td>\n",
       "      <td>-0.125532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.128995</td>\n",
       "      <td>-0.250886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.820878</td>\n",
       "      <td>-0.616209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.126707</td>\n",
       "      <td>-0.167044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.852911</td>\n",
       "      <td>-0.445554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2.706492</td>\n",
       "      <td>-0.021108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2.836494</td>\n",
       "      <td>0.200346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.683818</td>\n",
       "      <td>-0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-2.100322</td>\n",
       "      <td>-1.584042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.247844</td>\n",
       "      <td>-1.022232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>-1.192298</td>\n",
       "      <td>-0.613579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.915555</td>\n",
       "      <td>0.553752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.600062</td>\n",
       "      <td>-0.444573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>-1.533617</td>\n",
       "      <td>4.063913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>-1.844260</td>\n",
       "      <td>-1.586343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>-0.105793</td>\n",
       "      <td>-0.133109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1.983505</td>\n",
       "      <td>-0.058637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>-1.261572</td>\n",
       "      <td>3.464075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>-1.524347</td>\n",
       "      <td>-1.261217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index         0         1\n",
       "0       0 -0.883725  0.541541\n",
       "1       1  0.305743 -0.125532\n",
       "2       2 -0.128995 -0.250886\n",
       "3       3 -1.820878 -0.616209\n",
       "4       4  2.126707 -0.167044\n",
       "5       5  0.852911 -0.445554\n",
       "6       6  2.706492 -0.021108\n",
       "7       7  2.836494  0.200346\n",
       "8       8 -0.683818 -0.493562\n",
       "9       9 -2.100322 -1.584042\n",
       "10     10 -0.247844 -1.022232\n",
       "11     11 -1.192298 -0.613579\n",
       "12     12  1.915555  0.553752\n",
       "13     13  0.600062 -0.444573\n",
       "14     14 -1.533617  4.063913\n",
       "15     15 -1.844260 -1.586343\n",
       "16     16 -0.105793 -0.133109\n",
       "17     17  1.983505 -0.058637\n",
       "18     18 -1.261572  3.464075\n",
       "19     19 -1.524347 -1.261217"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_result = pca_result.reset_index()\n",
    "pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc1b75605f8>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca_result.plot.scatter(x=0, y=1,\n",
    "                      c='index',\n",
    "                      colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
